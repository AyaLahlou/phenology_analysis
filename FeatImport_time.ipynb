{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a937a90-32c8-4000-ba98-86e76f4d20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /burg/opt/anaconda3-2023.09/lib/python3.11/site-packages/singularity_hpc-0.1.28-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ruptures in /burg/home/al4385/.local/lib/python3.11/site-packages (1.1.9)\n",
      "Requirement already satisfied: numpy in /burg/opt/anaconda3-2023.09/lib/python3.11/site-packages (from ruptures) (2.0.0)\n",
      "Requirement already satisfied: scipy in /burg/opt/anaconda3-2023.09/lib/python3.11/site-packages (from ruptures) (1.11.4)\n",
      "Collecting numpy (from ruptures)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[33m  WARNING: The script f2py is installed in '/burg/home/al4385/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install ruptures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3068fb2c-6a0a-4309-a1a8-6e089693720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/opt/anaconda3-2023.09/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from typing import Dict,List,Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4619d5-74f4-47a8-8e50-fa9371d2ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/burg/glab/users/al4385/data/TFT_30/BDT_50_90.pickle'\n",
    "file_path = '/burg/glab/users/al4385/predictions/TFT_30_0429/pred_BDT_50_90.pkl'\n",
    "coord_path = '/burg/glab/users/al4385/data/coordinates/BDT_50_90.parquet'\n",
    "\n",
    "\n",
    "#file_path = \"predictions/predictions_BDT_boreal_jan6_timesplit.pkl\" #validation outputs \n",
    "with open(file_path, 'rb') as file:\n",
    "    # Load the dictionary from the pickle file\n",
    "    preds = pickle.load(file)\n",
    "\n",
    "#data_path = '../data/TFT_365test_03292024/BDT_50_90_test_NA.pickle'\n",
    "with open(data_path,'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "coords=pd.read_parquet(coord_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb93fd0-d1a2-4b41-8c60-6268afbb55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Index': data['data_sets']['test']['id'].flatten(),\n",
    "    'Flattened_Values': data['data_sets']['test']['target'].flatten(),\n",
    "})\n",
    "\n",
    "# Separate the channels into three separate arrays\n",
    "channel1 = preds['predicted_quantiles'][:, :, 0]\n",
    "channel2 = preds['predicted_quantiles'][:, :, 1]\n",
    "channel3 = preds['predicted_quantiles'][:, :, 2]\n",
    "\n",
    "#seperate weights into different channels \n",
    "df['tmin_weight_f'] = preds['future_selection_weights'][:, :, 0].flatten()\n",
    "df['tmax_weight_f'] = preds['future_selection_weights'][:, :, 1].flatten()\n",
    "df['rad_weight_f'] = preds['future_selection_weights'][:, :, 2].flatten()\n",
    "df['precip_weight_f'] = preds['future_selection_weights'][:, :, 3].flatten()\n",
    "df['photoperiod_weight_f'] = preds['future_selection_weights'][:, :, 4].flatten()\n",
    "df['SM_weight_f'] = preds['future_selection_weights'][:, :, 5].flatten()\n",
    "\n",
    "#df['tmin_weight_h'] = preds['historical_selection_weights'][:, :, 1].flatten()\n",
    "#df['tmax_weight_h'] = preds['historical_selection_weights'][:, :, 2].flatten()\n",
    "#df['rad_weight_h'] = preds['historical_selection_weights'][:, :, 3].flatten()\n",
    "#df['precip_weight_h'] = preds['historical_selection_weights'][:, :, 4].flatten()\n",
    "#df['photoperiod_weight_h'] = preds['historical_selection_weights'][:, :, 5].flatten()\n",
    "\n",
    "\n",
    "df['pred_01']= channel1.flatten()\n",
    "df['pred_05']= channel2.flatten()\n",
    "df['pred_09']= channel3.flatten()\n",
    "\n",
    "df[['location_id', 'time_id']] = df['Index'].str.split('_', n=1, expand=True)\n",
    "\n",
    "df['time_id'] = pd.to_datetime(df['time_id'])\n",
    "df['doy']=df['time_id'].dt.dayofyear\n",
    "df['location_id'] = df['location_id'].astype(int)\n",
    "df = df.sort_values(by=['location_id','time_id'])\n",
    "\n",
    "df['year']= df['time_id'].dt.year\n",
    "df['month']= df['time_id'].dt.month\n",
    "df['day']= df['time_id'].dt.day\n",
    "df = df.drop(columns=['Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f63407-6753-4874-9d59-481d0dd01dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = coords#['test']\n",
    "data_df = data_df.rename(columns={ 'location': 'location_id'})\n",
    "data_df = data_df.drop_duplicates()\n",
    "\n",
    "both = pd.merge(df, data_df, on='location_id', how='left')\n",
    "df_MAM=both"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
